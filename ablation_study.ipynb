{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72926c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOADING LOCAL MODEL] ./model-langanchor-finetuned\n",
      "[LOADING BASE MODEL FROM LOCAL] ./mT5_multilingual_XLSum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOADING LOCAL MODEL] ./model-langanchor-finetuned\n",
      "[LOADING BASE MODEL FROM LOCAL] ./mT5_multilingual_XLSum\n",
      "[LOADING LOCAL MODEL] ./model-langanchor-finetuned\n",
      "[LOADING BASE MODEL FROM LOCAL] ./mT5_multilingual_XLSum\n",
      "[LOADING LOCAL MODEL] ./model-langanchor-finetuned\n",
      "[LOADING BASE MODEL FROM LOCAL] ./mT5_multilingual_XLSum\n",
      "[LOADING LOCAL MODEL] ./model-lora-finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:Some weights of the model checkpoint at ./model-lora-finetuned were not used when initializing MT5ForConditionalGeneration: {'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 174\u001b[0m\n\u001b[0;32m    171\u001b[0m lora_scales \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m8\u001b[39m:\u001b[38;5;241m0.50\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m32\u001b[39m:\u001b[38;5;241m2.0\u001b[39m}\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r, scale \u001b[38;5;129;01min\u001b[39;00m lora_scales\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 174\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_local_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_LORA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     model \u001b[38;5;241m=\u001b[39m scale_lora_effect(model, scale)\n\u001b[0;32m    176\u001b[0m     res \u001b[38;5;241m=\u001b[39m evaluate_model(model)\n",
      "Cell \u001b[1;32mIn[2], line 52\u001b[0m, in \u001b[0;36mload_local_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     49\u001b[0m offload_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./offload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# You can change this to a suitable folder on your machine\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Load checkpoint and dispatch, passing offload_folder to handle large models\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint_and_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_split_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT5Block\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify offload folder here\u001b[39;49;00m\n\u001b[0;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\accelerate\\big_modeling.py:619\u001b[0m, in \u001b[0;36mload_checkpoint_and_dispatch\u001b[1;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes, force_hooks, strict)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\accelerate\\big_modeling.py:488\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    486\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\transformers\\modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2575\u001b[0m         )\n\u001b[1;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1369\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\torch\\nn\\modules\\module.py:955\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 955\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "File \u001b[1;32mc:\\Users\\TL1\\anaconda3\\envs\\aiml_labexam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1362\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1362\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1363\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1364\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1365\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PATH_VANILLA = \"./fine_tuned_mt5_summarization\"\n",
    "PATH_LORA    = \"./model-lora-finetuned\"\n",
    "PATH_LANG    = \"./model-langanchor-finetuned\"\n",
    "\n",
    "BASE_MODEL = \"./mT5_multilingual_XLSum\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def load_local_model(path):\n",
    "    print(f\"[LOADING LOCAL MODEL] {path}\")\n",
    "    config_path = os.path.join(path, \"config.json\")\n",
    "    if not os.path.isfile(config_path):\n",
    "        print(f\"Config.json not found at {path}. Creating config from base model...\")\n",
    "        base_config = AutoConfig.from_pretrained(BASE_MODEL)\n",
    "        base_config.save_pretrained(path)\n",
    "        print(f\"Config saved to {path}\")\n",
    "    config = AutoConfig.from_pretrained(path)\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "    offload_folder = \"./offload_model\"\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model,\n",
    "        path,\n",
    "        device_map=\"auto\",\n",
    "        no_split_module_classes=[\"T5Block\"],\n",
    "        dtype=torch.float16,\n",
    "        offload_folder=offload_folder\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_base_model():\n",
    "    print(f\"[LOADING BASE MODEL FROM LOCAL] {BASE_MODEL}\")\n",
    "    model_path = \"./mT5_multilingual_XLSum\"\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "rouge_m = evaluate.load(\"rouge\")\n",
    "bleu_m  = evaluate.load(\"sacrebleu\")\n",
    "bert_m  = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(preds, refs):\n",
    "    r = rouge_m.compute(predictions=preds, references=refs)\n",
    "    b = bleu_m.compute(predictions=preds, references=refs)\n",
    "    bs = bert_m.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "    return {\n",
    "        \"ROUGE-1\": r[\"rouge1\"],\n",
    "        \"ROUGE-2\": r[\"rouge2\"],\n",
    "        \"ROUGE-L\": r[\"rougeL\"],\n",
    "        \"BLEU\": b[\"score\"],\n",
    "        \"BERTScore\": float(np.mean(bs[\"f1\"])),\n",
    "    }\n",
    "\n",
    "def generate_summary(model, text):\n",
    "    x = tok(text, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "    y = model.generate(**x, max_length=120)\n",
    "    return tok.decode(y[0], skip_special_tokens=True)\n",
    "\n",
    "test_data = [\n",
    "    {\"text\": \"The Indian economy is growing steadily this year.\",\n",
    "     \"summary\": \"India's economy is rising.\"},\n",
    "    {\"text\": \"AI models require large datasets to perform well.\",\n",
    "     \"summary\": \"AI needs large datasets.\"},\n",
    "]\n",
    "\n",
    "def apply_lambda_effect(model, base_model, lam):\n",
    "    with torch.no_grad():\n",
    "        for p, q in zip(model.parameters(), base_model.parameters()):\n",
    "            p -= lam * (p - q)\n",
    "    return model\n",
    "\n",
    "def scale_lora_effect(model, scale):\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"lora\" in name.lower():\n",
    "                p *= scale\n",
    "    return model\n",
    "\n",
    "def freeze_encoder_layers(model, freeze_n):\n",
    "    for i, layer in enumerate(model.encoder.block):\n",
    "        if i < freeze_n:\n",
    "            layer.forward = lambda x, *args, **kwargs: x\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    preds, refs = [], []\n",
    "    for row in test_data:\n",
    "        preds.append(generate_summary(model, row[\"text\"]))\n",
    "        refs.append(row[\"summary\"])\n",
    "    return compute_metrics(preds, refs)\n",
    "\n",
    "results = []\n",
    "\n",
    "for lam in [0, 0.01, 0.05, 0.1]:\n",
    "    model = load_local_model(PATH_LANG)\n",
    "    base  = load_base_model()\n",
    "    model = apply_lambda_effect(model, base, lam)\n",
    "    res = evaluate_model(model)\n",
    "    res[\"Type\"] = f\"LangAnchor Î»={lam}\"\n",
    "    results.append(res)\n",
    "\n",
    "lora_scales = {4:0.25, 8:0.50, 16:1.0, 32:2.0}\n",
    "\n",
    "for r, scale in lora_scales.items():\n",
    "    model = load_local_model(PATH_LORA)\n",
    "    model = scale_lora_effect(model, scale)\n",
    "    res = evaluate_model(model)\n",
    "    res[\"Type\"] = f\"LoRA rank={r}\"\n",
    "    results.append(res)\n",
    "\n",
    "for fr in [0, 4, 6, 8]:\n",
    "    model = load_local_model(PATH_VANILLA)\n",
    "    model = freeze_encoder_layers(model, fr)\n",
    "    res = evaluate_model(model)\n",
    "    res[\"Type\"] = f\"Freeze {fr} layers\"\n",
    "    results.append(res)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ablation_results_local_xlsum.csv\", index=False)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_labexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
