{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99201b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\cuda\\__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 1: invalid argument (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, json, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23404aee810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL_DIR = \"./mT5_multilingual_XLSum\"     \n",
    "TRAIN_CSV      = \"summarization_dataset.csv\"    \n",
    "OUT_DIR        = \"model-langanchor-finetuned\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "EPOCHS            = 3\n",
    "BATCH_SIZE        = 8         \n",
    "LEARNING_RATE     = 2e-5\n",
    "WEIGHT_DECAY      = 0.01\n",
    "WARMUP_RATIO      = 0.06\n",
    "GRAD_ACCUM_STEPS  = 1         \n",
    "MAX_INPUT_LENGTH  = 512\n",
    "MAX_TARGET_LENGTH = 84\n",
    "FP16              = True      \n",
    "GRAD_CHKPT        = True      \n",
    "\n",
    "LAMBDA_ANCHOR     = 1.0       \n",
    "ANCHOR_BATCH_EVERY= 1        \n",
    "SAVE_EVERY_STEPS  = 1000      \n",
    "SEED              = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b4aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_DIR)\n",
    "if GRAD_CHKPT:\n",
    "    model.gradient_checkpointing_enable()\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "orig_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_DIR)\n",
    "orig_model.eval()\n",
    "orig_model.to(DEVICE)\n",
    "for p in orig_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Models loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50434dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1250)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WHITESPACE = lambda s: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', s.strip()))\n",
    "\n",
    "class SummDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_in=512, max_out=84):\n",
    "        self.inputs  = [WHITESPACE(x) for x in df[\"article\"].tolist()]\n",
    "        self.targets = [WHITESPACE(x) for x in df[\"highlights\"].tolist()]\n",
    "        self.tok = tokenizer\n",
    "        self.max_in = max_in\n",
    "        self.max_out= max_out\n",
    "    def __len__(self): return len(self.inputs)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(self.inputs[i], truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_in, return_tensors=\"pt\")\n",
    "        with self.tok.as_target_tokenizer():\n",
    "            lab = self.tok(self.targets[i], truncation=True, padding=\"max_length\",\n",
    "                           max_length=self.max_out, return_tensors=\"pt\")\n",
    "        labels = lab[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        return item\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "train_ds = SummDataset(df, tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "len(train_ds), len(train_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca7dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_PROMPTS = {\n",
    "    \"en\": \"This is a generic test sentence to anchor the language features.\",\n",
    "    \"hi\": \"यह भाषा की विशेषताओं को स्थिर करने के लिए एक सामान्य वाक्य है।\",\n",
    "    \"bn\": \"এটি ভাষার বৈশিষ্ট্য স্থির করতে একটি সাধারণ বাক্য।\",\n",
    "    \"ur\": \"یہ زبان کی خصوصیات کو برقرار رکھنے کے لیے ایک عمومی جملہ ہے۔\",\n",
    "    \"gu\": \"ભાષાની વિશેષતાઓને સ્થિર કરવા માટે આ એક સામાન્ય વાક્ય છે.\",\n",
    "    \"mr\": \"भाषेच्या वैशिष्ट्यांना स्थिर ठेवण्यासाठी हे एक सामान्य वाक्य आहे.\",\n",
    "    \"ta\": \"மொழியின் பண்புகளை உறுதிப்படுத்த இந்த ஒரு பொதுவான வாக்கியம்.\",\n",
    "    \"te\": \"భాష లక్షణాలను నిలుపుకునేందుకు ఇది ఒక సాధారణ వాక్యం.\",\n",
    "    \"kn\": \"ಭಾಷಾ ಲಕ್ಷಣಗಳನ್ನು ಸ್ಥಿರಗೊಳಿಸಲು ಇದು ಒಂದು ಸಾಮಾನ್ಯ ವಾಕ್ಯ.\",\n",
    "    \"ml\": \"ഭാഷയുടെ സവിശേഷതകളെ നിലനിർത്താൻ ഇത് ഒരു പൊതുവായ വാക്ക്യമാണ്.\",\n",
    "    \"pa\": \"ਭਾਸ਼ਾ ਦੇ ਗੁਣਾਂ ਨੂੰ ਕਾਇਮ ਰੱਖਣ ਲਈ ਇਹ ਇੱਕ ਸਧਾਰਣ ਵਾਕ ਹੈ।\",\n",
    "    \"ar\": \"هذه جملة عامة لتثبيت خصائص اللغة.\",\n",
    "    \"fa\": \"این یک جملهٔ عمومی برای پایدار کردن ویژگی‌های زبان است.\",\n",
    "    \"tr\": \"Dil özelliklerini sabitlemek için bu genel bir cümledir.\",\n",
    "    \"ru\": \"Это обычное предложение для закрепления языковых особенностей.\",\n",
    "    \"uk\": \"Це загальне речення для закріплення мовних особливостей.\",\n",
    "    \"fr\": \"Ceci est une phrase générique pour ancrer les caractéristiques de la langue.\",\n",
    "    \"es\": \"Esta es una frase genérica para anclar las características del idioma.\",\n",
    "    \"de\": \"Dies ist ein allgemeiner Satz, um die Spracheigenschaften zu verankern.\",\n",
    "    \"it\": \"Questa è una frase generica per ancorare le caratteristiche della lingua.\",\n",
    "    \"pt\": \"Esta é uma frase genérica para ancorar as características do idioma.\",\n",
    "    \"sw\": \"Hii ni sentensi ya jumla ya kuimarisha sifa za lugha.\",\n",
    "    \"yo\": \"Eyi jẹ gbolohun apapọ lati di awọn ẹya ede mọ.\",\n",
    "    \"am\": \"የቋንቋውን ባህሪያት ለማስቀመጥ ይህ አጠቃላይ አረፍተ ነገር ነው።\",\n",
    "    \"zh\": \"这是一个用于固定语言特征的通用句子。\",\n",
    "    \"ja\": \"言語の特徴を固定するための一般的な文です。\",\n",
    "    \"ko\": \"언어 특징을 고정하기 위한 일반적인 문장입니다.\",\n",
    "}\n",
    "\n",
    "anchor_texts = list(ANCHOR_PROMPTS.values())\n",
    "anchor_batch = tokenizer(anchor_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "anchor_ids   = anchor_batch[\"input_ids\"].to(DEVICE)\n",
    "anchor_mask  = anchor_batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    orig_enc = orig_model.get_encoder()(input_ids=anchor_ids, attention_mask=anchor_mask)\n",
    "    orig_hidden = orig_enc.last_hidden_state.detach()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2b7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "def anchor_loss(new_hidden, ref_hidden, mask):\n",
    "    m = mask.unsqueeze(-1).float()\n",
    "    diff2 = (new_hidden - ref_hidden) ** 2\n",
    "    num = (diff2 * m).sum()\n",
    "    den = m.sum().clamp(min=1.0)\n",
    "    return num / den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad22dee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TL1\\AppData\\Local\\Temp\\ipykernel_25392\\1311315063.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params = [\n",
    "    {\"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": [p for n,p in model.named_parameters() if     any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE)\n",
    "total_steps = math.ceil(len(train_dl) / GRAD_ACCUM_STEPS) * EPOCHS\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=FP16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c4e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1250 [00:00<?, ?it/s]c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\TL1\\AppData\\Local\\Temp\\ipykernel_25392\\3905920556.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=FP16):\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "Epoch 1/3:  80%|███████▉  | 999/1250 [10:18:14<2:24:05, 34.45s/it, loss=4.7001, main=1.3960, anchor=2.0522]c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6, 'no_repeat_ngram_size': 2}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 1250/1250 [12:42:21<00:00, 36.59s/it, loss=4.6027, main=1.7270, anchor=2.0255]  \n",
      "Epoch 2/3: 100%|██████████| 1250/1250 [11:59:59<00:00, 34.56s/it, loss=4.0399, main=1.9877, anchor=1.9100]  \n",
      "Epoch 3/3: 100%|██████████| 1250/1250 [11:51:57<00:00, 34.17s/it, loss=3.9201, main=1.9727, anchor=1.8414]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangAnchor fine-tuning complete; saved to: model-langanchor-finetuned\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(pbar, 1):\n",
    "        batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=FP16):\n",
    "           \n",
    "            out = model(**batch)\n",
    "            loss_main = out.loss\n",
    "\n",
    "            \n",
    "            if (global_step % ANCHOR_BATCH_EVERY) == 0:\n",
    "                new_enc = model.get_encoder()(input_ids=anchor_ids, attention_mask=anchor_mask)\n",
    "                new_hidden = new_enc.last_hidden_state\n",
    "                loss_anchor = anchor_loss(new_hidden, orig_hidden, anchor_mask)\n",
    "            else:\n",
    "                loss_anchor = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = loss_main + LAMBDA_ANCHOR * loss_anchor\n",
    "\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "        if (step % GRAD_ACCUM_STEPS) == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        running += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{running/step:.4f}\",\n",
    "                \"main\": f\"{loss_main.item():.4f}\",\n",
    "                \"anchor\": f\"{loss_anchor.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        if global_step % SAVE_EVERY_STEPS == 0:\n",
    "            ckpt_dir = os.path.join(OUT_DIR, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            model.save_pretrained(ckpt_dir)\n",
    "            tokenizer.save_pretrained(ckpt_dir)\n",
    "\n",
    "   \n",
    "    model.save_pretrained(os.path.join(OUT_DIR, f\"epoch-{epoch}\"))\n",
    "    tokenizer.save_pretrained(os.path.join(OUT_DIR, f\"epoch-{epoch}\"))\n",
    "\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "print(\"LangAnchor fine-tuning complete; saved to:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
