{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce1a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 584,170,752 || trainable%: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TL1\\AppData\\Local\\Temp\\ipykernel_26288\\1955212388.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 48:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.219500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('model-lora-finetuned\\\\tokenizer_config.json',\n",
       " 'model-lora-finetuned\\\\special_tokens_map.json',\n",
       " 'model-lora-finetuned\\\\spiece.model',\n",
       " 'model-lora-finetuned\\\\added_tokens.json',\n",
       " 'model-lora-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "model_name = \"google/mt5-base\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "data_files = {\"train\": \"summarization_dataset.csv\"}\n",
    "raw_dataset = load_dataset(\"csv\", data_files=data_files)[\"train\"]\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 84\n",
    "WHITESPACE_HANDLER = lambda text: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', text).strip())\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    inputs = [WHITESPACE_HANDLER(x) for x in batch[\"article\"]]\n",
    "    targets = [WHITESPACE_HANDLER(x) for x in batch[\"highlights\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels[\"input_ids\"] = [\n",
    "        [(lid if lid != tokenizer.pad_token_id else -100) for lid in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = raw_dataset.map(preprocess_batch, batched=True, remove_columns=raw_dataset.column_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(base_model, lora_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_lora)\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model-lora-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"logs-lora\",\n",
    "    report_to=\"none\",\n",
    "    run_name=\"lora-finetune\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# To resume from checkpoint use : trainer.train(resume_from_checkpoint=True) and comment above step.\n",
    "\n",
    "trainer.save_model(\"model-lora-finetuned\")\n",
    "tokenizer.save_pretrained(\"model-lora-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f517a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de47e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA model loaded successfully on cuda\n",
      "\n",
      "üåç [EN] Input:\n",
      "YouTube has announced a new policy to remove videos spreading false information \n",
      "    about vaccines. The company says it will take down content that claims vaccines cause \n",
      "    autism or infertility, and accounts of anti-vaccine influencers may be terminated.\n",
      "\n",
      "üßæ Summary:\n",
      "YouTube says it will remove videos spreading false information about vaccines.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üåç [HI] Input:\n",
      "‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§®‡§à ‡§®‡•Ä‡§§‡§ø ‡§ï‡•á ‡§§‡§π‡§§ ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§µ‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è \n",
      "    2025 ‡§§‡§ï 500 ‡§ó‡•Ä‡§ó‡§æ‡§µ‡•â‡§ü ‡§®‡§µ‡•Ä‡§ï‡§∞‡§£‡•Ä‡§Ø ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡§æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§∞‡§ñ‡§æ ‡§π‡•à‡•§\n",
      "\n",
      "üßæ Summary:\n",
      "‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§µ‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§à ‡§®‡•Ä‡§§‡§ø ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä ‡§π‡•à.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üåç [BN] Input:\n",
      "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶ï‡ßÉ‡¶∑‡¶ø ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶â‡¶¶‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡ßá‡¶∞ ‡¶´‡¶≤‡ßá ‡¶ß‡¶æ‡¶® ‡¶â‡ßé‡¶™‡¶æ‡¶¶‡¶® \n",
      "    ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡ßá‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\n",
      "\n",
      "üßæ Summary:\n",
      "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶ß‡¶æ‡¶® ‡¶â‡ßé‡¶™‡¶æ‡¶¶‡¶® ‡¶¨‡ßá‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üåç [SW] Input:\n",
      "Serikali ya Kenya imetangaza mpango mpya wa kusaidia wakulima wadogo \n",
      "    kuongeza uzalishaji wa chakula kupitia teknolojia za kisasa.\n",
      "\n",
      "üßæ Summary:\n",
      "Kenya imeanza kutumia teknolojia ya kisasa kuongeza uzalishaji wa chakula.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_name = \"./mT5_multilingual_XLSum\"          \n",
    "lora_model_path = \"./model-lora-finetuned\"             \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(\"LoRA model loaded successfully on\", device)\n",
    "\n",
    "texts = {\n",
    "    \"en\": \"\"\"YouTube has announced a new policy to remove videos spreading false information \n",
    "    about vaccines. The company says it will take down content that claims vaccines cause \n",
    "    autism or infertility, and accounts of anti-vaccine influencers may be terminated.\"\"\",\n",
    "\n",
    "    \"hi\": \"\"\"‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§®‡§à ‡§®‡•Ä‡§§‡§ø ‡§ï‡•á ‡§§‡§π‡§§ ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§µ‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è \n",
    "    2025 ‡§§‡§ï 500 ‡§ó‡•Ä‡§ó‡§æ‡§µ‡•â‡§ü ‡§®‡§µ‡•Ä‡§ï‡§∞‡§£‡•Ä‡§Ø ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡§æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§∞‡§ñ‡§æ ‡§π‡•à‡•§\"\"\",\n",
    "\n",
    "    \"bn\": \"\"\"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶ï‡ßÉ‡¶∑‡¶ø ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶â‡¶¶‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡ßá‡¶∞ ‡¶´‡¶≤‡ßá ‡¶ß‡¶æ‡¶® ‡¶â‡ßé‡¶™‡¶æ‡¶¶‡¶® \n",
    "    ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡ßá‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\"\"\",\n",
    "\n",
    "    \"sw\": \"\"\"Serikali ya Kenya imetangaza mpango mpya wa kusaidia wakulima wadogo \n",
    "    kuongeza uzalishaji wa chakula kupitia teknolojia za kisasa.\"\"\"\n",
    "}\n",
    "\n",
    "def summarize(text, lang):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=80,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\n [{lang.upper()}] Input:\\n{text}\\n\\n Summary:\\n{summary}\\n\" + \"-\"*80)\n",
    "\n",
    "for lang, text in texts.items():\n",
    "    summarize(text, lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63de16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
