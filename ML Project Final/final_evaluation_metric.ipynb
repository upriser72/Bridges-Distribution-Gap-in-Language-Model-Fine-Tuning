{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f91e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Running on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MT5Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Evaluating: mt5_base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loaded HF/Local model: ./mt5-multilingual-XLSum\n",
      "[en] → India's economy is at its highest rate in more than a decade.\n",
      "[fr] → Le prix du pétrole a chuté à un niveau record.\n",
      "[hi] → प्रधानमंत्री नरेंद्र मोदी ने नई शिक्षा नीति की घोषणा की है.\n",
      "[es] → El cambio climático está cambiando en todo el mundo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Evaluating: vanilla\n",
      "[en] →  Indian economy is growing steadily this year . Indian economy is growing steadily this year . Indian economy is growing steadily this year .\n",
      "[fr] →  Le marché international du pétrole a chuté récemment . Le marché international du pétrole a chuté récemment .\n",
      "[hi] →  PM 'tई शिक्षा नीति' घोषणा in UAE . PM 'tई शिक्षा नीति' घोषणा in UAE .\n",
      "[es] →  El clima is changing rápidamente en all el continent en all el continent . El clima is changing rápidamente en all el continent .\n",
      "\n",
      ">>> Evaluating: lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loaded PEFT adapter: ./model-lora-finetuned_2\n",
      "[en] → India's economy is at its highest rate in more than a decade.\n",
      "[fr] → Le prix du pétrole a chuté à un niveau record.\n",
      "[hi] → प्रधानमंत्री नरेंद्र मोदी ने नई शिक्षा नीति की घोषणा की है.\n",
      "[es] → El clima está cambiando rápidamente en todo el mundo.\n",
      "\n",
      ">>> Evaluating: langanchor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loaded HF/Local model: ./model-langanchor-finetuned\n",
      "[en] → The Indian economy is growing sharply in the past few years.\n",
      "[fr] → Le marché mondial du pétrole a chuté à un niveau record.\n",
      "[hi] → प्रधानमंत्री नरेंद्र मोदी ने नई शिक्षा नीति की घोषणा की है.\n",
      "[es] → El clima está cambiando rápidamente en todo el mundo.\n",
      "\n",
      "=== Final Results ===\n",
      "        Model  ROUGE-1  ROUGE-2  ROUGE-L     BLEU  BERTScore  Perplexity  \\\n",
      "0    mt5_base   0.4126   0.3676   0.4126  17.1513     0.9317      3.3336   \n",
      "1     vanilla   0.1477   0.0808   0.1477   2.6565     0.8726     11.7661   \n",
      "2        lora   0.3188   0.2604   0.3188  15.0482     0.9177      2.5572   \n",
      "3  langanchor   0.2326   0.1295   0.2326   7.2771     0.9079      2.2652   \n",
      "\n",
      "   Time(s)  \n",
      "0     9.06  \n",
      "1     6.98  \n",
      "2     7.96  \n",
      "3     8.98  \n"
     ]
    }
   ],
   "source": [
    "import os, torch, time, numpy as np, pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "except:\n",
    "    PeftModel = None\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\" Running on device:\", DEVICE)\n",
    "\n",
    "BASE_MODEL = \"./mT5-multilingual-XLSum\"\n",
    "HF_VANILLA = \"suryakantmani/mt5-vanilla-finetune-summarization\"\n",
    "\n",
    "\n",
    "tokenizer_vanilla = AutoTokenizer.from_pretrained(HF_VANILLA)\n",
    "vanilla_model_obj = AutoModelForSeq2SeqLM.from_pretrained(HF_VANILLA).to(DEVICE).eval()\n",
    "\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    \"mt5_base\": \"./mt5-multilingual-XLSum\",\n",
    "    \"vanilla\": HF_VANILLA,                \n",
    "    \"lora\": \"./model-lora-finetuned_2\",\n",
    "    \"langanchor\": \"./model-langanchor-finetuned\",\n",
    "}\n",
    "\n",
    "rouge_m = evaluate.load(\"rouge\")\n",
    "bleu_m = evaluate.load(\"sacrebleu\")\n",
    "bert_m = evaluate.load(\"bertscore\")\n",
    "\n",
    "test_data = [\n",
    "    {\"lang\": \"en\", \"text\": \"The Indian economy is growing steadily this year.\", \"summary\": \"India's economy is expanding.\"},\n",
    "    {\"lang\": \"fr\", \"text\": \"Le marché mondial du pétrole a chuté récemment.\", \"summary\": \"Le prix du pétrole a baissé.\"},\n",
    "    {\"lang\": \"hi\", \"text\": \"प्रधानमंत्री ने नई शिक्षा नीति की घोषणा की।\", \"summary\": \"नई शिक्षा नीति घोषित की गई।\"},\n",
    "    {\"lang\": \"es\", \"text\": \"El clima está cambiando rápidamente en todo el mundo.\", \"summary\": \"El cambio climático se acelera.\"},\n",
    "]\n",
    "\n",
    "# Model loader (supports direct HF, local, PEFT LoRA)\n",
    "def try_load_model(path):\n",
    "    try:\n",
    "        m = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "        print(\"  - Loaded HF/Local model:\", path)\n",
    "        return m\n",
    "    except:\n",
    "        if PeftModel is not None:\n",
    "            try:\n",
    "                base_m = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "                m = PeftModel.from_pretrained(base_m, path)\n",
    "                print(\"  - Loaded PEFT adapter:\", path)\n",
    "                return m\n",
    "            except:\n",
    "                print(\"  - Failed adapter load:\", path)\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "def do_summary(m, tok, txt, max_len=80):\n",
    "    enc = tok(txt, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out_ids = m.generate(**enc, max_length=max_len, num_beams=4)\n",
    "    return tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def get_ppl(m, tok, texts):\n",
    "    vals = []\n",
    "    for t in texts:\n",
    "        x = tok(t, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            l = m(**x, labels=x[\"input_ids\"]).loss\n",
    "        vals.append(torch.exp(l).item())\n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def evaluate_one(model_name, model_dir):\n",
    "    print(f\"\\n>>> Evaluating: {model_name}\")\n",
    "\n",
    "    if model_name == \"vanilla\":\n",
    "        m = vanilla_model_obj\n",
    "        tok = tokenizer_vanilla\n",
    "    else:\n",
    "\n",
    "        if os.path.exists(os.path.join(model_dir, \"config.json\")):\n",
    "            tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "        else:\n",
    "            tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "\n",
    "        m = try_load_model(model_dir)\n",
    "        if m is None:\n",
    "            print(\"  !! Skipping:\", model_name)\n",
    "            return None\n",
    "        m = m.to(DEVICE).eval()\n",
    "\n",
    "    preds, refs = [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for s in test_data:\n",
    "        p = do_summary(m, tok, s[\"text\"])\n",
    "        preds.append(p)\n",
    "        refs.append(s[\"summary\"])\n",
    "        print(f\"[{s['lang']}] → {p}\")\n",
    "\n",
    "    # Metrics\n",
    "    r = rouge_m.compute(predictions=preds, references=refs)\n",
    "    b = bleu_m.compute(predictions=preds, references=[[x] for x in refs])\n",
    "    bert_s = bert_m.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "    ppl_v = get_ppl(m, tok, [x[\"text\"] for x in test_data])\n",
    "    elapsed = round(time.time() - t0, 2)\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"ROUGE-1\": r[\"rouge1\"],\n",
    "        \"ROUGE-2\": r[\"rouge2\"],\n",
    "        \"ROUGE-L\": r[\"rougeL\"],\n",
    "        \"BLEU\": b[\"score\"],\n",
    "        \"BERTScore\": float(np.mean(bert_s[\"f1\"])),\n",
    "        \"Perplexity\": ppl_v,\n",
    "        \"Time(s)\": elapsed\n",
    "    }\n",
    "\n",
    "# Final evaluation\n",
    "all_rows = []\n",
    "for nm, pth in model_paths.items():\n",
    "    res = evaluate_one(nm, pth)\n",
    "    if res:\n",
    "        all_rows.append(res)\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(df.round(4))\n",
    "df.to_csv(\"multilingual_eval_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
