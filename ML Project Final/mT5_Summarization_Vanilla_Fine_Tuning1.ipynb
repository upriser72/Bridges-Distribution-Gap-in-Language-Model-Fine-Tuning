{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTeUs5tTPlOc"
      },
      "outputs": [],
      "source": [
        "!pip install torch datasets pandas transformers sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "def main():\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(\"Summarization_dataset.csv\", engine='python', on_bad_lines='skip')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'Summarization_dataset.csv' not found.\")\n",
        "        print(\"Please make sure the dataset file is uploaded to your Colab environment and the name matches exactly.\")\n",
        "        return\n",
        "\n",
        "    # Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "\n",
        "\n",
        "    model_name = \"google/mt5-base\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    prefix = \"summarize: \"\n",
        "    max_input_length = 512\n",
        "    max_target_length = 150\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        \"\"\"Tokenizes the dataset.\"\"\"\n",
        "\n",
        "        inputs = [prefix + str(article) for article in examples[\"article\"]]\n",
        "\n",
        "\n",
        "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        " .\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(examples[\"highlights\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "    )\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./results_mt5_summarization\",    # Directory to save the model and results\n",
        "        num_train_epochs=50,                   # Total number of training epochs\n",
        "        per_device_train_batch_size=2,         # Batch size per device during training\n",
        "        per_device_eval_batch_size=2,          # Batch size for evaluation (if used)\n",
        "        warmup_steps=50,                       # Number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,                     # Strength of weight decay\n",
        "        logging_dir='./logs_summarization',    # Directory for storing logs\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,                    # Only keep the last 2 saved models\n",
        "        predict_with_generate=True,            # Whether to use generate to calculate generative metrics\n",
        "        report_to=\"none\",                      # Disable integration with Weights & Biases\n",
        "    )\n",
        "\n",
        "    # Data collator prepares batches of data for the model.\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "    # The Trainer class handles the training and evaluation loop.\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # --- Start Fine-Tuning ---\n",
        "    print(\"Starting the fine-tuning process...\")\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning complete.\")\n",
        "\n",
        "    # -- Save the Fine-Tuned Model ---\n",
        "    final_model_path = \"./fine_tuned_mt5_summarization\"\n",
        "    trainer.save_model(final_model_path)\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "    print(f\"Model saved to {final_model_path}\")\n",
        "\n",
        "    # --- check result ---\n",
        "    print(\"\\n--- Running Inference with the Fine-Tuned Model ---\")\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    trained_model = MT5ForConditionalGeneration.from_pretrained(final_model_path)\n",
        "    trained_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "\n",
        "    # Define a new article to summarize\n",
        "    article_to_summarize = \"Scientists have discovered a new species of glowing frog in a remote rainforest. The frog, which emits a soft blue light, has unique bioluminescent properties that are not fully understood. Researchers believe this could lead to new advancements in medical imaging. The ecosystem where the frog was found is incredibly delicate and under threat from deforestation.\"\n",
        "\n",
        "    # Prepare the input for the model\n",
        "    prompt = f\"summarize: {article_to_summarize}\"\n",
        "    inputs = trained_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Generate the output\n",
        "    print(\"Generating summary...\")\n",
        "    outputs = trained_model.generate(\n",
        "        inputs,\n",
        "        max_length=150,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode and print the result\n",
        "    generated_text = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nOriginal Article:\")\n",
        "    print(article_to_summarize)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(generated_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bH4jW55GPsOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}