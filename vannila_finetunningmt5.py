# -*- coding: utf-8 -*-
"""Vannila_FinetunningMT5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZZLQpZd3EhJMSQ5n0KklkIUmNsLxXIO
"""

!pip install transformers datasets pandas scikit-learn

import pandas as pd
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

df = pd.read_csv('health_data_cleaned.csv')
df = df.rename(columns={'input': 'text', 'output': 'labels'})

df.head(3)

# 1. Fill NaN values with an empty string to prevent type errors.
df['text'] = df['text'].fillna('')
df['labels'] = df['labels'].fillna('')

# 2. Explicitly cast both columns to string type.
df['text'] = df['text'].astype(str)
df['labels'] = df['labels'].astype(str)

train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)

# Convert Pandas DataFrames to Hugging Face Dataset objects
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

# Create a DatasetDict
raw_datasets = DatasetDict({
    'train': train_dataset,
    'validation': val_dataset
})

from transformers import AutoTokenizer

# Choose your MT5 model (e.g., mt5-small)
model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Define max lengths (adjust based on your data)
max_input_length = 128
max_target_length = 64

def preprocess_function(examples):
    # --- 1. Tokenize Input (Source) ---
    # Add the task prefix to the input text
    inputs = [f"process: {ex}" for ex in examples["text"]]

    # Tokenize the input texts
    model_inputs = tokenizer(
        inputs,
        max_length=max_input_length,
        truncation=True
    )

    # --- 2. Tokenize Output (Labels/Target) ---
    # NOTE: For tokenizing the target text in a sequence-to-sequence model
    # using the Hugging Face `map` function (which handles the batching),
    # the recommended and correct approach is to use the `text_target`
    # argument in a separate tokenizer call.

    # Use text_target for the output/labels
    labels = tokenizer(
        text_target=examples["labels"], # Pass the list of target strings here
        max_length=max_target_length,
        truncation=True
    )

    # --- 3. Assign Labels ---
    # Assign the tokenized output to the crucial 'labels' key for training
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply the preprocessing function
tokenized_datasets = raw_datasets.map(
    preprocess_function,
    batched=True
)

from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Data collator to dynamically pad inputs and labels
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model
)

import torch
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./mt5_vanila_finetune_patient_Query", # Directory to save checkpoints
    num_train_epochs=3,                   # Number of training epochs
    per_device_train_batch_size=8,        # Batch size per device during training
    per_device_eval_batch_size=8,         # Batch size for evaluation
    warmup_steps=500,                     # Number of warmup steps for learning rate scheduler
    weight_decay=0.01,                    # Strength of weight decay
    logging_dir='./logs',                 # Directory for storing logs
    logging_steps=100,

    eval_strategy="epoch",                # Use 'eval_strategy' instead of 'evaluation_strategy'

    save_strategy="epoch",                # Save a checkpoint at the end of each epoch
    load_best_model_at_end=True,          # Load the best model found during training
    fp16=True if torch.cuda.is_available() else False, # Enable FP16 (recommended for faster training on GPU)
)

print("Seq2SeqTrainingArguments defined successfully.")

from transformers import Seq2SeqTrainer

# Optional: Import torch to check for CUDA
import torch

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Start the fine-tuning process
print("Fine Tunning Started")
trainer.train()
print("Fine Tunning Completed")

input = "Why my body is heat?"
prompt = f"process: {real_input}"

input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to('cuda:0')

# 3. Generate the response with controlled parameters
generated_ids = model.generate(
    input_ids,
    # Force the model to generate a substantial length
    max_new_tokens=100,
    # Use beam search to select the most probable long sequence
    num_beams=4,
    do_sample=False,
    # Ensure the model knows when to stop
    eos_token_id=tokenizer.eos_token_id
)

# 4. Decode the output and skip special tokens
decoded_output = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)

print(f"Input: {real_input}")
print(f"Output: {decoded_output}")

from huggingface_hub import notebook_login

notebook_login()

model.push_to_hub("suryakantmani/mt5-health-finetuned-Patient-Query")
tokenizer.push_to_hub("suryakantmani/mt5-health-finetuned-Patient-Query")

print("ðŸš€ Model uploaded to Hugging Face Hub successfully!")

