{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKf1USTWcwPt/wTwiAGaV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/upriser72/Bridges-Distribution-Gap-in-Language-Model-Fine-Tuning/blob/main/mT5_Summarization_Vanilla_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTeUs5tTPlOc"
      },
      "outputs": [],
      "source": [
        "!pip install torch datasets pandas transformers sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the fine-tuning process.\n",
        "    \"\"\"\n",
        "    # --- 1. Load and Prepare the Dataset ---\n",
        "    # Load the dataset from the CSV file.\n",
        "    # Using engine='python' to handle potential parsing errors in large CSV files.\n",
        "    # on_bad_lines='skip' will skip rows that have formatting issues.\n",
        "    try:\n",
        "        df = pd.read_csv(\"Summarization_dataset.csv\", engine='python', on_bad_lines='skip')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'Summarization_dataset.csv' not found.\")\n",
        "        print(\"Please make sure the dataset file is uploaded to your Colab environment and the name matches exactly.\")\n",
        "        return\n",
        "\n",
        "    # Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # --- 2. Load Tokenizer and Model ---\n",
        "    model_name = \"google/mt5-small\"\n",
        "    # The tokenizer is responsible for converting text into a format the model can understand.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # The model is the pre-trained mT5-small architecture.\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # --- 3. Preprocess the Data ---\n",
        "    # We need to format the input and output correctly for the T5 model.\n",
        "    # The model will be trained to generate the 'highlights' from the 'article' column.\n",
        "    prefix = \"summarize: \"\n",
        "    max_input_length = 512\n",
        "    max_target_length = 150 # Adjusted for summarization\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        \"\"\"Tokenizes the dataset.\"\"\"\n",
        "        # Create the combined input text for each item in the batch\n",
        "        inputs = [prefix + str(article) for article in examples[\"article\"]]\n",
        "\n",
        "        # Tokenize the inputs\n",
        "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Tokenize the targets (outputs)\n",
        "        # The 'with tokenizer.as_target_tokenizer():' block is essential for T5-style models.\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(examples[\"highlights\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    # Apply the preprocessing function to the entire dataset\n",
        "    tokenized_dataset = dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "    )\n",
        "\n",
        "    # --- 4. Set Up Training ---\n",
        "    # Define training arguments. These control various aspects of the training process.\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./results_mt5_summarization\",    # Directory to save the model and results\n",
        "        num_train_epochs=50,                   # Total number of training epochs\n",
        "        per_device_train_batch_size=2,         # Batch size per device during training\n",
        "        per_device_eval_batch_size=2,          # Batch size for evaluation (if used)\n",
        "        warmup_steps=50,                       # Number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,                     # Strength of weight decay\n",
        "        logging_dir='./logs_summarization',    # Directory for storing logs\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,                    # Only keep the last 2 saved models\n",
        "        predict_with_generate=True,            # Whether to use generate to calculate generative metrics\n",
        "        report_to=\"none\",                      # Disable integration with Weights & Biases\n",
        "    )\n",
        "\n",
        "    # Data collator prepares batches of data for the model.\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "    # The Trainer class handles the training and evaluation loop.\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # --- 5. Start Fine-Tuning ---\n",
        "    print(\"Starting the fine-tuning process...\")\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning complete.\")\n",
        "\n",
        "    # --- 6. Save the Fine-Tuned Model ---\n",
        "    final_model_path = \"./fine_tuned_mt5_summarization\"\n",
        "    trainer.save_model(final_model_path)\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "    print(f\"Model saved to {final_model_path}\")\n",
        "\n",
        "    # --- 7. Inference Example ---\n",
        "    print(\"\\n--- Running Inference with the Fine-Tuned Model ---\")\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    trained_model = MT5ForConditionalGeneration.from_pretrained(final_model_path)\n",
        "    trained_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "\n",
        "    # Define a new article to summarize\n",
        "    article_to_summarize = \"Scientists have discovered a new species of glowing frog in a remote rainforest. The frog, which emits a soft blue light, has unique bioluminescent properties that are not fully understood. Researchers believe this could lead to new advancements in medical imaging. The ecosystem where the frog was found is incredibly delicate and under threat from deforestation.\"\n",
        "\n",
        "    # Prepare the input for the model\n",
        "    prompt = f\"summarize: {article_to_summarize}\"\n",
        "    inputs = trained_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Generate the output\n",
        "    print(\"Generating summary...\")\n",
        "    outputs = trained_model.generate(\n",
        "        inputs,\n",
        "        max_length=150,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode and print the result\n",
        "    generated_text = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nOriginal Article:\")\n",
        "    print(article_to_summarize)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(generated_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bH4jW55GPsOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}