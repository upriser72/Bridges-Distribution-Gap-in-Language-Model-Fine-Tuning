from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import evaluate
import torch
import re

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Use multilingual translation model
model_name = "facebook/m2m100_418M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Load ROUGE metric
rouge = evaluate.load("rouge")

def normalize_text(text):
    text = text.strip()
    text = re.sub(r"[।\.\,\!\?]", "", text)
    text = text.lower()
    return text

# Test data (same as before)
data = [
    {"src_lang": "en", "tgt_lang": "hi", "input": "The weather is good today.", "expected": "आज मौसम अच्छा है"},
    {"src_lang": "en", "tgt_lang": "mr", "input": "The weather is good today.", "expected": "आज हवामान छान आहे"},
    {"src_lang": "hi", "tgt_lang": "en", "input": "मैं स्कूल जा रहा हूँ", "expected": "I am going to school"},
    {"src_lang": "mr", "tgt_lang": "en", "input": "मी पाणी पित आहे", "expected": "I am drinking water"},
    {"src_lang": "hi", "tgt_lang": "mr", "input": "मैं बाजार जा रहा हूँ", "expected": "मी बाजारात जात आहे"},
    {"src_lang": "mr", "tgt_lang": "hi", "input": "मी घरी जात आहे", "expected": "मैं घर जा रहा हूँ"},
]

def translate(sample):
    src, src_lang, tgt_lang = sample["input"], sample["src_lang"], sample["tgt_lang"]
    tokenizer.src_lang = src_lang
    tokenizer.tgt_lang = tgt_lang
    inputs = tokenizer(src, return_tensors="pt", padding=True).to(device)
    forced_token_id = tokenizer.get_lang_id(tgt_lang)
    with torch.no_grad():
        outputs = model.generate(**inputs, forced_bos_token_id=forced_token_id, max_length=50)
    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return pred

def evaluate_rouge():
    preds, refs = [], []
    for sample in data:
        pred = translate(sample)
        preds.append(normalize_text(pred))
        refs.append(normalize_text(sample["expected"]))
        print(f"{sample['src_lang']} → {sample['tgt_lang']}")
        print(f"Input: {sample['input']}")
        print(f"Predicted: {pred}")
        print(f"Expected: {sample['expected']}\n{'-'*40}")

    # Compute ROUGE scores
    results = rouge.compute(predictions=preds, references=refs)
    return results

if _name_ == "_main_":
    scores = evaluate_rouge()
    print("\n=== ROUGE SCORES ===")
    for metric, value in scores.items():
        print(f"{metric.upper():10s}: {value:.4f}")
