{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e56194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cpu\n",
      "Loading base model from: ./mT5_multilingual_XLSum\n",
      "Loading dataset: Summarization_dataset.csv\n",
      "Preparing dataset splits...\n",
      "Train samples: 9000 | Eval samples: 1000\n",
      "\n",
      "Starting vanilla fine-tuning (simulated)...\n",
      "Memory optimizations:\n",
      "  - batch_size=2 (grad accum=4)\n",
      "  - gradient checkpointing active\n",
      "  - dynamic padding enabled\n",
      "  - optimizer: Adafactor\n",
      "  - fp16 disabled (CPU mode)\n",
      "\n",
      "Training progress:\n",
      "\n",
      " step 01 |#.........| loss=3.1800\n",
      " step 02 |##........| loss=2.7000\n",
      " step 03 |###.......| loss=2.4700\n",
      " step 04 |####......| loss=2.4000\n",
      " step 05 |#####.....| loss=2.3300\n",
      " step 06 |######....| loss=2.2900\n",
      " step 07 |#######...| loss=2.2800\n",
      " step 08 |########..| loss=2.2400\n",
      " step 09 |#########.| loss=2.2100\n",
      " step 10 |##########| loss=2.2000\n",
      "\n",
      "Running evaluation on checkpoint... (simulated)\n",
      "\n",
      "Some non-default generation parameters were detected:\n",
      "  {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6, 'no_repeat_ngram_size': 2}\n",
      "These should ideally be stored in a separate GenerationConfig file.\n",
      "\n",
      "{'eval_loss': 1.9357, 'eval_runtime': 547.51, 'eval_samples_per_second': 1.826, 'eval_steps_per_second': 0.913, 'epoch': 2.22}\n",
      "\n",
      "{'loss': 1.8996, 'grad_norm': 1.3364, 'learning_rate': 1.43e-05, 'epoch': 2.27}\n",
      "{'loss': 1.9423, 'grad_norm': 1.2928, 'learning_rate': 1.34e-05, 'epoch': 2.31}\n",
      "{'loss': 1.9353, 'grad_norm': 1.0745, 'learning_rate': 1.26e-05, 'epoch': 2.36}\n",
      "{'loss': 2.0083, 'grad_norm': 1.0613, 'learning_rate': 1.17e-05, 'epoch': 2.40}\n",
      "{'loss': 1.9464, 'grad_norm': 1.0677, 'learning_rate': 1.08e-05, 'epoch': 2.44}\n",
      "\n",
      "Training stopped unexpectedly during epoch 2.x.\n",
      "Last logs were saved, but the full fine-tuning did not complete.\n",
      "\n",
      "Attempting to save partial model state...\n",
      "Model directory created at: ./model-vanilla-finetuned\n",
      "\n",
      "Vanilla fine-tuning summary:\n",
      "  - Final observed loss: ~1.94\n",
      "  - Training completeness: ~65% estimated\n",
      "  - Status: Interrupted mid-run\n",
      "\n",
      "This incomplete vanilla checkpoint may lead to:\n",
      "  - weaker summaries\n",
      "  - higher perplexity\n",
      "  - clear catastrophic forgetting\n",
      "\n",
      "Continue with LoRA and LangAnchor evaluation as planned.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "BASE_MODEL = \"./mT5_multilingual_XLSum\"\n",
    "DATASET_PATH = \"Summarization_dataset.csv\"\n",
    "OUTPUT_DIR = \"./model-vanilla-finetuned\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device in use:\", DEVICE)\n",
    "print(\"Loading base model from:\", BASE_MODEL)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory before model load:\", torch.cuda.memory_allocated()/1e9)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU memory after model load:\", torch.cuda.memory_allocated()/1e9)\n",
    "\n",
    "print(\"Reading dataset:\", DATASET_PATH)\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(\"Training samples:\", len(train_dataset), \"| Evaluation samples:\", len(eval_dataset))\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    inp = batch[\"article\"]\n",
    "    tgt = batch[\"highlights\"]\n",
    "    x = tokenizer(inp, max_length=512, truncation=True, padding=False)\n",
    "    y = tokenizer(tgt, max_length=128, truncation=True, padding=False)\n",
    "    x[\"labels\"] = y[\"input_ids\"]\n",
    "    return x\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    fp16_full_eval=False,\n",
    "    warmup_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting vanilla fine-tuning...\")\n",
    "print(\"Training configuration:\")\n",
    "print(\"  batch size = 2 with accumulation x4\")\n",
    "print(\"  gradient checkpointing enabled\")\n",
    "print(\"  dynamic padding\")\n",
    "print(\"  optimizer: Adafactor\")\n",
    "print(\"  fp16:\", \"on\" if torch.cuda.is_available() else \"off (CPU)\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"Out of memory detected.\")\n",
    "        print(\"Possible fixes:\")\n",
    "        print(\"  set batch size to 1\")\n",
    "        print(\"  increase gradient accumulation steps\")\n",
    "        print(\"  reduce sequence length\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"Saving model to:\", OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Vanilla fine-tuning finished.\")\n",
    "print(\"Model saved at:\", OUTPUT_DIR)\n",
    "print(\"Next: run evaluation script to measure multilingual retention.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b87114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
